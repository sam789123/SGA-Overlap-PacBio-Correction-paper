\chapter{Literature Review}

\section{Next Generation Sequencing}

Over the past few years, there has been a fundamental change from the Sanger sequencing in genome analysis\cite{Maxam1977,Sanger1977}. Prior to this departure, the Sanger sequencing had dominated the industry for almost twenty years and led to many significant accomplishments, including the completion of the only finished-grade human genome sequence\cite{HGP2004}. In spite of many technical improvements during this era, the limitations of Sanger sequencing showed a demand for new and improved technologies for sequencing large numbers of human genomes. Development of new methods has been a new target to achieve, leaving Sanger sequencing with fewer reported advances. The Sanger sequencing is considered as a 'first-generation' technology, and newer methods are referred to as next-generation sequencing (NGS)\cite{Mardis2008}. These newer technologies involve various strategies that rely on a combination of template preparation, sequencing and imaging, and genome alignment and assembly methods. The arrival of NGS technologies has changed the way we think about scientific approaches and clinical research in basic. The ability to produce an enormous volume of data cheaply is the major advance offered by NGS - in some cases more than one billion short reads per cyclic run\cite{Wheeler2008}. Determining the order of bases is not the only thing to be considered for the reason that the feature of NGS expands the realm of experimentation. The ability to sequence the whole genome of many individuals in a population has allowed large-scale comparative and evolutionary studies to be implemented that were unimaginable just a few years ago. Here, we will present a technical review of template preparation to provide guidance on how these technologies work.

\section{Third Generation Sequencing}
Third-generation sequencing has two main characteristics comparison with the next generation sequencing~\cite{pmid22829749}. First, PCR isn't needed before sequencing, which shortens DNA preparation time for sequencing. Second, the signal is captured in real time so that the signal is monitored during the enzymatic reaction. For example, Pacific Bioscience released their first commercial third-generation sequencing instrument, PacBio RS: a real-time, single-molecule sequencer. It takes 4 to 6 hours instead of days to prepare the sample and it also does not need PCR step in preparation step, which reduces the bias and error caused by PCR. Furthermore, the average read length is up to 1300 bp, which is longer than second-generation sequencing technology. These technologies are quite useful for $de novo$ genome and transcriptome assembly as they have the potentail to resolve complex repeats and span entire gene transcripts. However, this instrument generates reads that average only $82.1\%-84.6\%$~\cite{pmid21142692, pmid21793740} nucleotide accuracy, with uniformly distributed errors dominated by point insertions and deletions~\cite{Koren2012}. This high error rates obscures the alignments between reads and complicates analysis since the pairwise differences between two reads is approximately twice their individual error rates, and is far beyond the $5\%–10\%$ error rates~\cite{Marcel2005, pmid18952627, pmid22147368} that most genome assemblers can tolerate—simply increasing the alignment sensitivity of traditional assemblers is computationally infeasible. Additionally, the PacBio technology utilizes hairpin adaptors for sequencing double stranded DNA, which can result in chimeric reads if the sequencing reaction processes both strands of the DNA (first in the forward and then reverse direction). While it is possible to generate accurate sequences on the PacBio RS by reading a circularized molecule multiple times (circular consensus or CCS), this approach reduces read length by a factor equal to the number of times the molecule is traversed, resulting in much shorter reads (e.g. median = 423 bp, max= 1,915 bp). Thus, there is a great potential advantage to the long, single-pass reads if the error rates can be algorithmically managed~\cite{Koren2012}.


\section{Sequencing error}
A sequencing error or mis-call occurs when a sequencing method calls one or more bases incorrectly, leading to an inaccurate read. Due to the vagaries of molecular biology, no laboratory-based DNA sequencing methods are perfectly precise; they are all known to mis-call bases occasionally in the machines. The chance of a sequencing error is generally known and quantifiable, thanks to extensive testing and calibration of the sequencing machines. Each base in a read is assigned a quality score, indicating confidence that the base has been called correctly. Some sequencing methods are more reliable than others and so give higher quality scores. Sequencing errors are also more likely to appear at the end of a read, far from where the insert has begun, so quality scores there are typically lower~\cite{sequencing_error}.

\subsection{Types of sequencing errors}
\begin{description}

\item [Mismatch.]
A mismatch is a substitution of one base for another, e.g., an A for a C. Mismatches are different from SNPs, which are actual differences in the genome (due to polymorphism). It is not easy to distinguish mismatches from SNPs, especially at low coverage. Mismatches are often fixed during error correction.

\item [Indel.]
An indel, short for "insertion/deletion", occurs when a read contains a different number of bases from its reference at some points in the alignment. An insertion occurs when the read contains extra bases, while a deletion occurs when the read is missing bases. Indels, like mismatches, may actually be true indicators of polymorphism rather than the result of sequencing errors.
\end{description}

\section{FM-index}
In computer science, an FM-index is a compressed full-text substring index based on the Burrows-Wheeler transform, with some similarities to the suffix array. It was created by Paolo Ferragina and Giovanni Manzini~\cite{Ferragina2000}, who describe it as an opportunistic data structure as it allows compression of the input text while still permitting fast substring queries. The name stands for Full-text index in Minute space~\cite{Ferragina2005}.It can be used to efficiently find the number of occurrences of a pattern within the compressed text, as well as locate the position of each occurrence. Both the query time and storage space requirements are sublinear with respect to the size of the input data. The original authors have devised improvements to their original approach and dubbed it "FM-Index version 2". A further improvement, the alphabet-friendly FM-index, combines the use of compression boosting and wavelet trees~\cite{Ferragina2004} to significantly reduce the space usage for large alphabets~\cite{wiki:FM-index}. The FM-index has found use in, among other places, bioinformatics~\cite{pmid20529929}.